{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7624dd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prishagupta/Desktop/learning/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4b353fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HistoryAwareChatbot:\n",
    "    def __init__(self, api_key: str, model_name: str = \"gemini-1.5-flash\"):\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Configure Google GenAI client (for token counting)\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.client = genai\n",
    "\n",
    "        # Use Gemini via LangChain\n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "            model=model_name,\n",
    "            google_api_key=api_key,\n",
    "            temperature=0.7,\n",
    "            max_output_tokens=2048,\n",
    "        )\n",
    "\n",
    "        # LangChain memory: remembers all interactions\n",
    "        self.memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)\n",
    "\n",
    "        # Combine LLM + Memory for a conversational chain\n",
    "        self.chain = ConversationChain(llm=self.llm, memory=self.memory)\n",
    "\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def chat(self, user_input: str) -> dict:\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        self.conversation_history.append(\n",
    "            {\"role\": \"user\", \"content\": user_input, \"timestamp\": timestamp}\n",
    "        )\n",
    "        try:\n",
    "            response = self.chain.run(user_input)\n",
    "            self.conversation_history.append(\n",
    "                {\"role\": \"assistant\", \"content\": response, \"timestamp\": datetime.now().isoformat()}\n",
    "            )\n",
    "            return {\n",
    "                \"response\": response,\n",
    "                \"conversation_length\": len(self.conversation_history),\n",
    "                \"timestamp\": timestamp,\n",
    "                \"tokens_used\": self.count_tokens(),   # NEW: show token usage\n",
    "            }\n",
    "        except Exception as e:\n",
    "            error_response = f\"Error: {str(e)}\"\n",
    "            self.conversation_history.append(\n",
    "                {\"role\": \"assistant\", \"content\": error_response, \"timestamp\": datetime.now().isoformat()}\n",
    "            )\n",
    "            return {\n",
    "                \"response\": error_response,\n",
    "                \"error\": str(e),\n",
    "                \"conversation_length\": len(self.conversation_history),\n",
    "                \"timestamp\": timestamp,\n",
    "            }\n",
    "\n",
    "    def count_tokens(self) -> int:\n",
    "        \"\"\"Count tokens used so far in the conversation\"\"\"\n",
    "        try:\n",
    "            contents = [\n",
    "                {\"role\": h[\"role\"], \"parts\": [h[\"content\"]]}\n",
    "                for h in self.conversation_history\n",
    "            ]\n",
    "            count = self.client.models.count_tokens(\n",
    "                model=self.model_name,\n",
    "                contents=contents\n",
    "            )\n",
    "            return count.total_tokens\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Token count failed: {e}\")\n",
    "            return -1\n",
    "\n",
    "    def get_conversation_summary(self) -> str:\n",
    "        if not self.conversation_history:\n",
    "            return \"No conversation history available.\"\n",
    "        summary_prompt = \"Summarize this conversation: \" + json.dumps(\n",
    "            self.conversation_history, indent=2\n",
    "        )\n",
    "        try:\n",
    "            # Direct summary from Gemini\n",
    "            return self.llm.invoke(summary_prompt).content\n",
    "        except Exception as e:\n",
    "            return f\"Unable to generate summary: {str(e)}\"\n",
    "\n",
    "    def clear_conversation_history(self):\n",
    "        self.conversation_history = []\n",
    "        self.memory.clear()\n",
    "        print(\"Conversation history cleared\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d4365f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    API_KEY = \"AIzaSyAi-X-2AF0MYVtowJX2cVUU5OtIHzKJsRo\"  \n",
    "    chatbot = HistoryAwareChatbot(api_key=API_KEY)\n",
    "    print(\"ü§ñ Gemini History-Aware Chatbot ready!\")\n",
    "    while True:\n",
    "        user_input = input(\"\\nüë§ You: \").strip()\n",
    "        if not user_input:\n",
    "            continue\n",
    "        if user_input.lower() == \"quit\":\n",
    "            break\n",
    "        elif user_input.lower() == \"summary\":\n",
    "            print(\"\\nüìù Summary:\", chatbot.get_conversation_summary())\n",
    "            continue\n",
    "        elif user_input.lower() == \"clear\":\n",
    "            chatbot.clear_conversation_history()\n",
    "            continue\n",
    "        elif user_input.lower() == \"tokens\":\n",
    "            print(f\"\\nüî¢ Tokens used so far: {chatbot.count_tokens()}\")\n",
    "            continue\n",
    "        result = chatbot.chat(user_input)\n",
    "        print(f\"\\nü§ñ Assistant: {result['response']}\")\n",
    "        print(f\"üî¢ Tokens so far: {result['tokens_used']}\")\n",
    "    print(\"\\nüëã Goodbye!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f978e62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
