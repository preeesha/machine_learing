{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0dbe694",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238888a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel '.venv (Python 3.11.9)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, load_diabetes, make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171ad8bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel '.venv (Python 3.11.9)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dcfc38",
   "metadata": {},
   "source": [
    "Decision trees are hierarchical, tree-like structures used for:\n",
    "        • Classification: Predicting categorical outcomes\n",
    "        • Regression: Predicting continuous values\n",
    "        \n",
    "        Structure:\n",
    "        • Root Node: Starting point with entire dataset\n",
    "        • Internal Nodes: Decision points with splitting criteria\n",
    "        • Leaf Nodes: Final predictions/outcomes\n",
    "        • Branches: Paths connecting nodes based on conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9360d32b",
   "metadata": {},
   "source": [
    "        Algorithm Steps:\n",
    "        • Start with root node containing all data\n",
    "        • Find best feature and threshold to split data\n",
    "        • Create child nodes based on split\n",
    "        • Repeat recursively until stopping criteria met\n",
    "        • Assign predictions to leaf nodes\n",
    "        \n",
    "        Key Concepts:\n",
    "        • Splitting Criteria: How to choose best split\n",
    "        • Impurity Measures: Quantify node homogeneity\n",
    "        • Pruning: Prevent overfitting\n",
    "        • Tree Depth: Control complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cf6921",
   "metadata": {},
   "source": [
    "Classification Impurity Measures:\n",
    "        \n",
    "        a) Gini Impurity:\n",
    "           Gini = 1 - Σ(p_i²) where p_i is probability of class i\n",
    "           • Range: [0, 1-1/k] where k is number of classes\n",
    "           • 0 = pure node (all samples same class)\n",
    "           • Higher values = more impure\n",
    "        \n",
    "        b) Entropy:\n",
    "           Entropy = -Σ(p_i * log2(p_i))\n",
    "           • Range: [0, log2(k)]\n",
    "           • 0 = pure node\n",
    "           • log2(k) = maximum impurity\n",
    "        \n",
    "        c) Information Gain:\n",
    "           IG = Parent_Impurity - Σ(|S_v|/|S| * Child_v_Impurity)\n",
    "           • Measures reduction in impurity after split\n",
    "           • Higher IG = better split\n",
    "        \n",
    "        Regression Impurity Measures:\n",
    "        \n",
    "        a) Mean Squared Error (MSE):\n",
    "           MSE = (1/n) * Σ(y_i - y_mean)²\n",
    "           • Measures variance within node\n",
    "        \n",
    "        b) Mean Absolute Error (MAE):\n",
    "           MAE = (1/n) * Σ|y_i - y_median|\n",
    "           • Less sensitive to outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d10487",
   "metadata": {},
   "source": [
    "Best Split Selection:\n",
    "        \n",
    "        1. For each feature:\n",
    "           - Sort unique values\n",
    "           - Consider each value as potential threshold\n",
    "           - Calculate impurity for resulting split\n",
    "        \n",
    "        2. Choose split that maximizes:\n",
    "           - Information Gain (classification)\n",
    "           - Variance reduction (regression)\n",
    "        \n",
    "        3. Stopping Criteria:\n",
    "           - Maximum tree depth reached\n",
    "           - Minimum samples per node\n",
    "           - Minimum impurity decrease\n",
    "           - All samples in node belong to same class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b7f75c",
   "metadata": {},
   "source": [
    "DISADVANTAGES OF DECISION TREES\n",
    "\n",
    "        ✗ Prone to overfitting (high variance)\n",
    "        ✗ Unstable (small changes in data → different trees)\n",
    "        ✗ Can create biased trees if classes are imbalanced\n",
    "        ✗ May not generalize well to unseen data\n",
    "        ✗ Limited to axis-parallel splits\n",
    "        ✗ Can be computationally expensive for large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9891e5",
   "metadata": {},
   "source": [
    "        Pre-pruning (Early Stopping):\n",
    "        • max_depth: Maximum tree depth\n",
    "        • min_samples_split: Minimum samples to split node\n",
    "        • min_samples_leaf: Minimum samples in leaf node\n",
    "        • min_impurity_decrease: Minimum impurity reduction\n",
    "        \n",
    "        Post-pruning (Cost Complexity Pruning):\n",
    "        • Remove subtrees that don't improve performance\n",
    "        • Balance tree complexity vs. accuracy\n",
    "        • Use validation set to determine optimal pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b53f96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
